[
  {
    "objectID": "report/segmentation_demo.html",
    "href": "report/segmentation_demo.html",
    "title": "Blog",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import tree\n\nog_df = pd.read_csv(\"https://msalicedatapublic.z5.web.core.windows.net/datasets/Pricing/pricing_sample.csv\")\nog_df.columns = og_df.columns.str.replace(' ', '_').str.lower()  # Clean column names\n\n\ndef show_tree(cp_val):\n    reg_tree = DecisionTreeRegressor(ccp_alpha=cp_val)\n    reg_tree.fit(reg_tree_data.drop(columns='demand'), reg_tree_data['demand'])\n\n    plt.figure(figsize=(20,10))\n    tree.plot_tree(reg_tree, filled=True, feature_names=reg_tree_data.drop(columns='demand').columns)\n    plt.show()\n\npricing_df = og_df.copy()\n\nreg_tree_data = pricing_df.drop(columns=['price', 'has_membership', 'is_us'])\n\nshow_tree(5)\n\n\n\n\n\n\n\n\n\nshow_tree(1)\n\n\n\n\n\n\n\n\n\ncp_val = 1\nreg_tree = DecisionTreeRegressor(ccp_alpha=cp_val)\nreg_tree.fit(reg_tree_data.drop(columns='demand'), reg_tree_data['demand'])\n\nDecisionTreeRegressor(ccp_alpha=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(ccp_alpha=1) \n\n\n\npricing_df\n\n\n  \n    \n\n\n\n\n\n\naccount_age\nage\navg_hours\ndays_visited\nfriends_count\nhas_membership\nis_us\nsongs_purchased\nincome\nprice\ndemand\n\n\n\n\n0\n3\n53\n1.834234\n2\n8\n1\n1\n4.903237\n0.960863\n1.0\n3.917117\n\n\n1\n5\n54\n7.171411\n7\n9\n0\n1\n3.330161\n0.732487\n1.0\n11.585706\n\n\n2\n3\n33\n5.351920\n6\n9\n0\n1\n3.036203\n1.130937\n1.0\n24.675960\n\n\n3\n2\n34\n6.723551\n0\n8\n0\n1\n7.911926\n0.929197\n1.0\n6.361776\n\n\n4\n4\n30\n2.448247\n5\n8\n1\n0\n7.148967\n0.533527\n0.8\n12.624123\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n2\n56\n6.095439\n3\n10\n0\n0\n7.406568\n0.545599\n0.8\n9.447720\n\n\n9996\n3\n52\n7.828183\n7\n5\n0\n1\n0.782374\n0.944415\n0.8\n15.314092\n\n\n9997\n1\n27\n6.527350\n6\n9\n0\n1\n10.926441\n0.815953\n1.0\n11.263675\n\n\n9998\n4\n49\n2.803943\n6\n9\n1\n1\n4.205016\n0.504313\n0.8\n12.801971\n\n\n9999\n5\n35\n9.334821\n2\n9\n0\n1\n2.971992\n1.456270\n1.0\n21.667410\n\n\n\n\n10000 rows × 11 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\npricing_df['leaf'] = reg_tree.apply(reg_tree_data.drop(columns='demand'))\n\n# Elasticity function\ndef own_price_reg(leaf_num):\n    df  = pricing_df[pricing_df['leaf'] == leaf_num]\n    model = smf.ols('demand ~ price * has_membership * is_us', data=df).fit()\n    return model.params['price']\n\nelasticities = [own_price_reg(leaf) for leaf in pricing_df['leaf'].unique()]\nprint(elasticities)\n\n[-18.265984327571324, -18.17187119271577, -5.235805786421214, -1.4067338547890245]"
  },
  {
    "objectID": "posts/post1/index.html",
    "href": "posts/post1/index.html",
    "title": "Market Segmentation with Regression Trees in Python: A Step-by-Step Guide",
    "section": "",
    "text": "In the world of data science, one of the most powerful tools for understanding customer behavior and improving marketing strategies is market segmentation. By dividing a market into subgroups with shared characteristics, companies can optimize pricing, enhance customer satisfaction, and allocate resources effectively. In this post, we explore how to use regression trees in Python for market segmentation, providing a step-by-step guide for practical application.\n\nWhat is Market Segmentation?\n\nMarket/Customer segmentation a important tool for identifying “who” your target audience is, serving as a key component of your business model. It divides a market into distinct customer groups with similar needs, interests, and priorities, allowing businesses to tailor marketing efforts and products. This process enhances resource allocation and customer satisfaction. For example, a retailer might segment by geography, income, or behavior.\nFor this exercise, we’ll use user behavior and characteristics (avgerage hours, days visisted, income, etc..) to predict demand elasticities with a regression tree model.\nCustomer segmentation in a business model offers several advantages:\n\n1. Effective Marketing Strategy\nSegmentation deepens understanding of customers, allowing businesses to prioritize marketing channels and target specific customer groups with tailored messages and creatives.\n\n\n2. Predicting Customer Behavior\nBy classifying customers based on behavior, segmentation helps predict future actions, enabling more proactive strategies.\n\n\n3. Personalized Customer Experience\nSegmentation allows businesses to offer services tailored to individual customer needs based on their data and create targeted programs, content, and incentives to reward customers and maintain their satisfaction and engagement.\n\n\n4. Improved Conversion Metrics\nSegmentation boosts conversion rates by enabling personalized strategies. It also helps identify abandoned carts and encourages checkout or purchase.\n(Takyar 2024)\n\n\n\nKey Concepts to Understand\n\n1. Regression Trees\nA machine learning tool that predicts continuous outcomes by splitting data based on input features.\n\n\n2. CP Value\nComplexity Parameter decides how deep the decision tree will be grown into. If any split does not increase the overall R2 of the model by at least cp, the tree does not split said branch any further\n\n\n\nSample Dataset\nThe dataset is from (Keith Battocchi 2019) developed by, Alice Project.\nThere are around 10,000 observations and 9 continuous and categorical variables representing user’s behaviors and characteristics.\nDescription of varaibales are as following:\n\n\n\n\n\n\n\nFeature Name\nDetails\n\n\n\n\naccount_age\nuser’s account age\n\n\nage\nuser’s age\n\n\navg_hours\nthe average hours user was online per week in the past\n\n\ndays_visited\nthe average number of days user visited the website per week\n\n\nfriend_count\nnumber of friends of user’s account\n\n\nhas_membership\nwhether the user had membership\n\n\nis_US\nwhether the user accesses the website from the United States\n\n\nsongs_purchased\nthe average songs user purchased per week (non-discount season)\n\n\nincome\nuser’s income\n\n\nprice\nthe price user was exposed to during the discount season (baseline price * small discount)\n\n\ndemand\nsongs user purchased during the discount season\n\n\n\n\n\nStep-by-Step Guide\n\nStep 1: Loading and Preparing the Data\nFirst, we load the libraries and data, and perform some basic data cleaning.\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import tree\n\nog_df = pd.read_csv(\"https://msalicedatapublic.z5.web.core.windows.net/datasets/Pricing/pricing_sample.csv\")\nog_df.columns = og_df.columns.str.replace(' ', '_').str.lower()  # Clean column names\n\n\nStep 2: Choose number of segments\nIn this step, we use a regression tree to identify how many segments (or clusters) we want. The tree’s complexity is controlled by a hyperparameter (ccp_alpha), which we adjust to visualize the tree structure at different levels of complexity.\ndef show_tree(cp_val, data):\n    reg_tree = DecisionTreeRegressor(ccp_alpha=cp_val)\n    reg_tree.fit(data.drop(columns='demand'), data['demand'])\n\n    plt.figure(figsize=(20,10))\n    tree.plot_tree(reg_tree, filled=True, feature_names=data.drop(columns='demand').columns)\n    plt.show()\n\npricing_df = og_df.copy()\n\n# exclude variable of intersts for segmentation\nreg_tree_data = pricing_df.drop(columns=['price'])\nshow_tree(5, reg_tree_data)\n\nshow_tree(1, reg_tree_data)\n\nWe want 4 segments, so we will proceed with cp value of 1.\n\n\nStep 3: Building the Regression Tree\nIn this step, we build the actual regression tree model, training it on the user characteristics to predict the demand.\ncp_val = 1\nreg_tree = DecisionTreeRegressor(ccp_alpha=cp_val)\nreg_tree.fit(reg_tree_data.drop(columns='demand'), reg_tree_data['demand'])\nBy fitting the model, we are able to identify the relationships between the various user characteristics and demand, and the tree automatically creates splits (segments) based on these variables.\n\n\nStep 4: Assigning Users to Leaves\nNext, we assign each user to a regression tree leaf:\npricing_df['leaf'] = reg_tree.apply(reg_tree_data.drop(columns='demand'))\nThis step adds a new variable leaf to the dataset, which indicates the leaf (or segment) that each user belongs to.\n\n\nStep 5: Interpreting the Results\nAt this point, we have successfully segmented the users based on their characteristics. Each user is now assigned to a segment (leaf) that represents a group of users with similar characteristics.\nTo calculate price elasticities for each segment, do the following:\npricing_df['log_price'] = np.log(pricing_df['price'])\npricing_df['log_q'] = np.log(pricing_df['demand'])\ndata = []\n\n# Elasticity function\ndef own_price_reg(leaf_num):\n    df  = pricing_df[pricing_df['leaf'] == leaf_num]\n    model = smf.ols('log_q ~ log_price', data=df).fit()\n    return model.params['log_price']\n\nfor leaf in pricing_df['leaf'].unique():\n    own_price = own_price_reg(leaf)\n    leaf_data = pricing_df[pricing_df['leaf'] == leaf]\n    avg_values = leaf_data.drop(columns=['leaf']).mean().to_dict()\n    \n    # Add the result to the data list\n    avg_values['leaf'] = leaf\n    avg_values['own_price_reg'] = own_price\n    data.append(avg_values)\n\npd.DataFrame(data)\nBoth price and demand are transformed by log. This allows the coefficients to represent elasticity (the percentage change in demand for a percentage change in price).\n\n\n\n\nTable 1: Average features and elasticity by segmentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nindex\naccount_age\nage\navg_hours\ndays_visited\nfriends_count\nhas_membership\nis_us\nsongs_purchased\nincome\nprice\ndemand\nlog_price\nlog_q\nleaf\nown_price_reg\n\n\n\n\n0\n3.01726\n38.9238\n5.07327\n1.94948\n10.0314\n0.49294\n0.800439\n5.10132\n0.697376\n0.867587\n7.78766\n-0.145771\n2.02731\n2\n-2.14204\n\n\n1\n2.97933\n39.4955\n5.05556\n5.99788\n9.97509\n0.514043\n0.786963\n4.98314\n0.702922\n0.869104\n12.753\n-0.144278\n2.53648\n3\n-1.27903\n\n\n2\n2.99316\n38.5003\n4.97012\n6.00526\n10.07\n0.50868\n0.793793\n5.14311\n1.5587\n0.958127\n24.6107\n-0.0454373\n3.20234\n6\n-0.112743\n\n\n3\n3.01388\n38.797\n4.93473\n2.01388\n10.0212\n0.488926\n0.807273\n4.9841\n1.57671\n0.958446\n19.592\n-0.0450707\n2.9738\n5\n-0.1203\n\n\n\n\n\n\n\n\nGroup 2:\n\nPrice Elasticity (-2.14): Highly elastic. The demand is sensitive to price changes. A small price increase could significantly reduce demand.\nAverage Income (0.70): Relatively low compared to other groups, suggesting higher price sensitivity (income effect).\nAverage Days Visited (1.95): Low platform engagement, possibly contributing to higher elasticity as these users might not be habitual buyers.\nInterpretation: Price adjustments could significantly impact demand in this group, making them ideal for sale offerings.\n\nGroup 3:\n\nPrice Elasticity (-1.28): Moderately elastic. Users are still responsive to price changes, but less so than Group 0.\nAverage Income (0.70): Similar to Group 2. Moderate price sensitivity.\nAverage Days Visited (5.99): Higher engagement than Group 0, which could reduce elasticity as these users are more invested in the platform.\nInterpretation: This group may respond to price changes, but their higher engagement suggests potential for retention despite price increases.\n\nGroup 6:\n\nPrice Elasticity (-0.11): Nearly inelastic. The demand is relatively insensitive to price changes.\nIncome (1.56): Higher than the other groups, likely contributing to lower price sensitivity.\nAverage Days Visited (6.01): Highest engagement. This could reduce elasticity as these users are more invested in the platform. highly.\nInterpretation: This group can tolerate higher prices without significant reductions in demand, making them ideal for premium offerings.\n\nGroup 5:\n\nPrice Elasticity (-0.12): Nearly inelastic. Similar to Group 6.\nIncome (1.58): Comparable to Group 6, supporting lower price sensitivity.\nAverage Days Visited (2.01): Despite low platform engagement, demand (19.59) remains high, which could mean that users in this group value the product highly.\nInterpretation: Like Group 6, this group is less price-sensitive and may represent another target for premium pricing or tailored offers.\n\nRecommendation:\n\nHigh Elasticity Groups (2 and 3):\n\nThese groups are more price-sensitive due to lower income, engagement, or both.\nPrice reductions or promotions may drive significant demand increases.\n\nLow Elasticity Groups (6 and 5):\n\nThese groups show low sensitivity to price changes, likely due to higher income and inherent demand.\nThey are suitable candidates for price increases or premium-tier products.\n\n\n\nBy understanding these nuances, pricing and marketing strategies can be tailored to maximize revenue while maintaining user satisfaction.\n\n\n\nReal-World Applications\nMarket segmentation using regression trees can be applied to a variety of industries. Here are a few examples:\n\nRetail: Optimize promotions and product offerings based on customer demographics.\nHealthcare: Segment patients to tailor treatment plans.\nFinance: Offer personalized financial products based on income and behavior.\n\n\n\nConclusion\nIn this blog post, we demonstrated how to segment a market using regression trees in Python. By using regression trees for segmentation, you can uncover actionable insights to drive strategy and decision-making. Whether refining pricing strategies or identifying customer needs, these tools offer a robust way to make the most of your data.\nIf you’re interested in diving deeper into regression trees or market segmentation, try applying these techniques to your own datasets and explore how different groups behave differently in your industry.\n\n\n\n\n\n\n\n\n\nReferences\n\nKeith Battocchi, Maggie Hei, Eleanor Dillon. 2019. “EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation.” https://github.com/py-why/EconML.\n\n\nTakyar, Akash. 2024. “How to Build a Machine Learning Model for Customer Segmentation?” LeewayHertz. https://www.leewayhertz.com/build-a-machine-learning-model-for-customer-segmentation/#How-to-build-a-machine-learning-model-for-customer-segmentation."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "👋 Hi there! I’m Yeji, a passionate data scientist with a knack for machine learning and predictive modeling. My repositories showcase a variety of projects, from web scraping and causal inference to Dockerfile and Github Actions practicies. I love exploring new technologies and applying them to solve real-world problems.\nThanks for stopping by! 😊"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yeji Sohn",
    "section": "",
    "text": "Market Segmentation with Regression Trees in Python: A Step-by-Step Guide\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nYeji Sohn\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Classification Models in Health Context\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nYeji Sohn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post2/index.html",
    "href": "posts/post2/index.html",
    "title": "Comparing Classification Models in Health Context",
    "section": "",
    "text": "Recent developments in machine learning and artificial intelligence have introduced breakthrough methods for predicting diagnoses, health emergencies, and more. Despite some skepticism, the adoption of these technologies is steadily increasing. Healthcare is an industry constantly evolving with new technologies and treatments, making it challenging for professionals to keep up.\nTimely diagnoses enabled by machine learning can help healthcare providers make informed decisions, allocate resources effectively, and ultimately save lives. Traditional diagnostic methods, which often rely on manual interpretation of clinical test results, can be time-consuming, subjective, and prone to errors. As health data becomes more accessible, machine learning has emerged as a powerful tool for diagnosing and predicting diseases, including heart disease.\nIn this overview, we explore two popular machine learning methods—generalized linear models and tree models—that are commonly applied in healthcare."
  },
  {
    "objectID": "posts/post2/index.html#understanding-the-models",
    "href": "posts/post2/index.html#understanding-the-models",
    "title": "Comparing Classification Models in Health Context",
    "section": "Understanding the Models",
    "text": "Understanding the Models\n\n1. Logistic Regression: Predicting Yes or No\nDefinition\nLogistic Regression is a supervised learning model designed to solve binary classification problems—situations where the answer is a clear “yes” or “no.” For example, it can predict whether a person has a heart disease (True/1) or not (False/0).\nHow It Works\nThe outcome of logistic regression is a probability, which always falls between 0 and 1. To get there, it uses log-odds \\(\\text{logit}(p_i)\\) and applies a logistic function:\n\\[\n  \\text{logit}(p_i) = \\ln\\left(\\frac{p_i}{1 - p_i}\\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_k X_k\n  \\]\nHere, \\(p_i\\) is the probability of success (e.g., having heart disease), and \\(1-p_i\\) is the probability of failure.\n\\(X_1, X_2, \\dots, X_k\\) are the independent variables (like age or cholesterol level).\nThe \\(\\beta\\) values are coefficients determined using a method called maximum likelihood estimation (MLE).\nA probability less than 0.5 predicts the outcome as 0 (False), while a probability greater than 0.5 predicts it as 1 (True). For example, if \\(\\beta_1 = 0.8\\), it means that for every 1-unit increase in \\(X_1\\), the odds of success increase by \\(e^{0.8} \\approx 2.22\\)—a 122% increase!\nFor more information, (see Ibm 2024b).\n\n\n2. Decision Tree: Breaking It Down into Steps\nDefinition\nA Decision Tree is a versatile, non-parametric algorithm (no assumptions on data distributions or form) that can be used for both classification and regression. It uses a tree-like structure to make decisions, splitting data into branches based if-else statements.\nHow It Works\nAt each level of the tree, the algorithm chooses the best feature (like age or income) and splits the data to minimize impurity.\nA node represents a decision based on a feature and a leaf contains the final prediction.\nThe process continues recursively until all data points are classified, or a stopping rule (like a minimum node size) is met.\nFor example, imagine classifying patients based on age and resting blood pressure. A Decision Tree might first split by whether patient is more than 50 years old, then by whether their blood pressure is high. Each split narrows down the data to more specific groups.\n\n\n\nDecision Tree Flowchart\n\n\nFor more information, (see Ibm 2024a)"
  },
  {
    "objectID": "posts/post2/index.html#comparison-of-logistic-regression-and-decision-tree",
    "href": "posts/post2/index.html#comparison-of-logistic-regression-and-decision-tree",
    "title": "Comparing Classification Models in Health Context",
    "section": "Comparison of Logistic Regression and Decision Tree",
    "text": "Comparison of Logistic Regression and Decision Tree\nWhen deciding between Logistic Regression and Decision Trees, it’s important to consider factors like complexity, interpretability, data handling, and how well they generalize.\n\n1. Model Complexity\n\nLogistic Regression:\nLogistic Regression is straightforward and focuses on simplicity. It’s perfect when you need a model that’s easy to set up, understand, and interpret.\nDecision Tree:\nDecision Trees are more complex because they take a greedy approach—splitting data repeatedly to find the best outcomes. This makes them computationally heavier and more time-consuming to train. However, the payoff is a highly visual and intuitive representation of the decision-making process.\n\n\n\n2. Interpretability\n\nLogistic Regression:\nThis model gives you coefficients that show how much each feature (like age or income) contributes to the outcome. For example, a high coefficient for age could indicate its strong influence in predicting heart disease.\nDecision Tree:\nThe paths in a Decision Tree mirror how decisions are made in real life: “If X, then Y.” These paths are simple to follow, and the visual representation makes it easier to explain the results. Plus, features ranked by importance help you see which ones matter most in decision-making.\n\n\n\n3. Handling of Data\n\nLogistic Regression:\nLogistic Regression assumes a lot about your data. It expects a linear relationship between the predictors and the log-odds of the outcome. It also works best when the data is independent and without multicollinearity (strong correlations between predictors).\nDecision Tree:\nDecision Trees thrive on flexibility. They don’t require data to follow specific distributions or assumptions, and they handle both numerical and categorical data effortlessly. This makes them ideal for messy, real-world datasets.\n\n\n4. Overfitting and Generalization\n\nLogistic Regression:\nLogistic Regression tends to generalize well, which means it performs consistently on new, unseen data. This is because it’s a simpler model with fewer parameters, making it less likely to overfit.\nDecision Tree:\nDecision Trees are prone to overfitting because they can become overly complex, capturing even minor details (or noise) in the training data. However, methods like pruning (cutting back unnecessary branches) and cross-validation (testing on different subsets of data) can help mitigate this. Also, small changes in the data can result in a very different tree, highlighting their high variance."
  },
  {
    "objectID": "posts/post2/index.html#clinical-implications-heart-disease-example",
    "href": "posts/post2/index.html#clinical-implications-heart-disease-example",
    "title": "Comparing Classification Models in Health Context",
    "section": "Clinical Implications: Heart Disease Example",
    "text": "Clinical Implications: Heart Disease Example\nIn this section, we compare the performance of decision trees and logistic regression in predicting heart disease. To see how each model works in practice, check out this Example\n\n1. Predictive Power\nLogistic Regression emerged as the stronger model, achieving an F-1 score of 82% compared to the Decision Tree’s 70%. This higher performance means Logistic Regression was better at balancing two critical errors: false positives (unnecessary worry and treatment) and false negatives (missed diagnoses).\nKey features identified by both models, such as cholesterol levels, blood pressure, and age, align closely with established clinical indicators for cardiovascular health. These insights are not just statistically significant—they’re actionable. For example, a high cholesterol level flagged by the model could prompt earlier interventions like medication or lifestyle adjustments, potentially preventing severe outcomes.\n\n\n2. Practical Applications\nBoth logistic regression and decision trees have practical applications in clinical settings, but they offer different strengths and limitations when used in real-world healthcare.\nLogistic regression is often favored in settings where the relationship between features and outcomes is assumed to be linear. It is relatively easy to implement and interpret, which is particularly valuable in clinical environments where practitioners may need to quickly understand and trust model outputs. For example, logistic regression might be used in primary care to predict heart disease risk based on basic health metrics like age, blood pressure, and cholesterol levels. However, logistic regression can struggle with capturing more complex, non-linear relationships between features.\nDecision trees are particularly useful when clinicians want a model that can handle both numerical and categorical data and uncover non-linear relationships. They are highly interpretable and provide a clear flowchart that explains the decision process (e.g., “If age &gt; 60 and cholesterol &gt; 240, then high risk of heart disease”). This can be incredibly useful in a clinical setting where healthcare providers need to follow a transparent decision-making process. However, decision trees are prone to overfitting, meaning they may perform well on training data but less accurately on unseen data. They also tend to struggle with small data sets or those with lots of noise.\nLogistic regression offers the benefit of simplicity and ease of interpretation. Its limitations lie in its inability to model complex interactions between features, making it less effective in cases where the relationship between variables is non-linear. On the other hand, decision tree excel at modeling complex, non-linear relationships, making them highly flexible in predicting outcomes based on a mix of clinical features. However, they can be prone to overfitting and may require pruning or other adjustments to improve generalizability.\nIn practice, the choice between logistic regression and decision trees often comes down to the specific needs of the clinical application (whether interpretability or model flexibility is more important), the nature of the data at hand, and test scores."
  },
  {
    "objectID": "posts/post2/index.html#conclusion",
    "href": "posts/post2/index.html#conclusion",
    "title": "Comparing Classification Models in Health Context",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we have explored the use of two simple machine learning models—Logistic Regression and Decision Trees—in clinical research, particularly for predicting heart disease. We have examined how each model functions, their strengths and weaknesses, and their practical applications in healthcare settings.\nThe future of machine learning in healthcare holds significant promise. As more diverse and comprehensive healthcare data becomes available, we anticipate more sophisticated models capable of handling increasingly complex relationships between patient characteristics and disease outcomes. Further advancements in hybrid models—combining the strengths of both Logistic Regression and Decision Trees—may also emerge to offer a balance between interpretability and predictive power.\nMoreover, with the rise of deep learning and other advanced algorithms, we may see an expansion of machine learning applications in areas like medical imaging, drug discovery, and personalized treatment plans. Ethical considerations and transparency will remain key as these technologies evolve, ensuring they enhance, rather than replace, human decision-making in clinical practice. Continuous research and development will help refine these models, making them more accurate, accessible, and beneficial for improving patient outcomes across various healthcare domains."
  }
]